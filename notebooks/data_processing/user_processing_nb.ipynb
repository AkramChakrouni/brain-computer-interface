{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "import torch\n",
    "import pywt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    # Read the input file\n",
    "    df_in = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get the header row\n",
    "    header_row = list(df_in.columns)\n",
    "\n",
    "    # Create a DataFrame from the header row\n",
    "    header_df = pd.DataFrame([header_row], columns=df_in.columns)\n",
    "\n",
    "    # Concatenate the header row with the original DataFrame\n",
    "    df_imm = pd.concat([header_df, df_in], ignore_index=True)\n",
    "\n",
    "    # Reset column names\n",
    "    df_imm.columns = range(df_imm.shape[1])\n",
    "\n",
    "    # Remove the first column\n",
    "    df_imm = df_imm.drop(0, axis=1)\n",
    "\n",
    "    # Shape is (n_timepoints, n_channels + target)\n",
    "    return df_imm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bandpass_filter(signal, b, a):\n",
    "    return filtfilt(b, a, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(df_in):\n",
    "    # Remove the last column, to shape (n_channels, n_timepoints)\n",
    "    df_imm = df_in.iloc[:, :-1].T\n",
    "    \n",
    "    # Last column is the target_column\n",
    "    target_column = df_in.iloc[:, -1]\n",
    "    \n",
    "    # Change to float32\n",
    "    df_imm = df_imm.astype(np.float32)\n",
    "    \n",
    "    # Filter parameters\n",
    "    l_freq = 8\n",
    "    h_freq = 30\n",
    "    order = 5\n",
    "    fs = 250\n",
    "\n",
    "    nyquist = 0.5 * fs\n",
    "    low = l_freq / nyquist\n",
    "    high = h_freq / nyquist\n",
    "\n",
    "    # Calculate filter coefficients\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    \n",
    "    # Apply the filter to each row independently\n",
    "    for i in range(df_imm.shape[0]):\n",
    "        df_imm.iloc[i] = apply_bandpass_filter(df_imm.iloc[i], b, a)\n",
    "        \n",
    "    # Add the target_column column back, with no column name\n",
    "    df_filtered = df_imm.T\n",
    "    df_filtered[''] = target_column\n",
    "    \n",
    "    # reset column names\n",
    "    df_filtered.columns = range(df_filtered.shape[1])\n",
    "    \n",
    "    # Shape is (n_timepoints, n_channels + target_column)\n",
    "    return df_filtered    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(df):\n",
    "    current_sequence = df.iloc[0, -1]  # Get the first value in the last column\n",
    "    current_dataset = df.iloc[[0]]  # Initialize the first dataset\n",
    "    datasets = []\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        if df.iloc[i, -1] == current_sequence:  # If the sequence continues\n",
    "            current_dataset = pd.concat([current_dataset, df.iloc[[i]]])\n",
    "        else:  # If the sequence changes\n",
    "            datasets.append(current_dataset)\n",
    "            current_sequence = df.iloc[i, -1]\n",
    "            current_dataset = df.iloc[[i]]\n",
    "\n",
    "    datasets.append(current_dataset)  # Append the last dataset\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epochs_and_labels(dataset):\n",
    "    # Get only the data, by removing the last column\n",
    "    data = dataset.drop(dataset.columns[-1], axis=1).T\n",
    "\n",
    "    # Reset column names\n",
    "    data.columns = range(data.shape[1])\n",
    "\n",
    "    # Split data in epochs of 0.5 second, sampled at 250 Hz, so 125 samples per epoch\n",
    "    epoch_length = 126  # 0.5 seconds * 250 Hz\n",
    "    n_samples = data.shape[1]\n",
    "\n",
    "    # Calculate the number of full epochs\n",
    "    n_epochs = n_samples // epoch_length\n",
    "\n",
    "    # Discard remaining samples that do not fit into a full epoch\n",
    "    data = data.iloc[:, :n_epochs * epoch_length]\n",
    "\n",
    "    # Properly slice the data into epochs\n",
    "    epochs = [data.iloc[:, i*epoch_length:(i+1)*epoch_length] for i in range(n_epochs)]\n",
    "\n",
    "    # Convert list of DataFrames to 3D numpy array\n",
    "    epochs = np.stack([epoch.values for epoch in epochs], axis=0)\n",
    "\n",
    "    # Get the category column\n",
    "    category = dataset[dataset.columns[-1]]\n",
    "\n",
    "    # Label is 0 is the first index of category 'L' and 1 if the first index of category 'R'\n",
    "    label = 0 if category.iloc[0] == 'L' else 1\n",
    "\n",
    "    # Create a label array with the same length as the number of epochs\n",
    "    labels = np.full((n_epochs,), label)\n",
    "\n",
    "    return epochs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(epoch):    \n",
    "    for i in range(epoch.shape[0]):\n",
    "        channel_epoch = epoch[i, :]\n",
    "        mean = np.mean(channel_epoch)\n",
    "        std = np.std(channel_epoch)\n",
    "        z_scored_epoch = (channel_epoch - mean) / std\n",
    "        epoch[i, :] = z_scored_epoch\n",
    "    \n",
    "    return epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_to_scale(freq, wavelet='morl', sampling_rate=250):\n",
    "    # For the Morlet wavelet, scales are inversely proportional to frequency\n",
    "    center_freq = pywt.central_frequency(wavelet)\n",
    "    return center_freq / (freq / sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wavelet_transform(data_norm, wavelet='morl', freq_range=(8, 30), sampling_rate=250):\n",
    "    \"\"\"\n",
    "    Apply wavelet transform to EEG data.\n",
    "    \n",
    "    Parameters:\n",
    "    data_norm (ndarray): 2D array with shape (channels, time_points)\n",
    "    wavelet (str): Wavelet type (default 'morl')\n",
    "    freq_range (tuple): Frequency range for the CWT (default (8, 30) Hz)\n",
    "    sampling_rate (int): Sampling rate of the EEG data (default 250 Hz)\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: 3D array with shape (channels, scales, time_points)\n",
    "    \"\"\"\n",
    "    n_channels, n_times = data_norm.shape\n",
    "    # Define scales based on the desired frequency range\n",
    "    scales = frequency_to_scale(np.arange(freq_range[0], freq_range[1]+1), wavelet=wavelet, sampling_rate=sampling_rate)\n",
    "    \n",
    "    coeffs = []\n",
    "    for i in range(n_channels):\n",
    "        # Compute the wavelet transform coefficients\n",
    "        coef, _ = pywt.cwt(data_norm[i], scales=scales, wavelet=wavelet)\n",
    "        coeffs.append(coef)\n",
    "    \n",
    "    # Stack coefficients to form a 3D tensor\n",
    "    coeffs_done = np.stack(coeffs, axis=0)\n",
    "    \n",
    "    return coeffs_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Process a user specific dataset to create a tensor dataset and labels tensor.\n",
    "\n",
    "    Arguments:\n",
    "        - file_path (str): The path to the input file.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df_in = load_dataset(file_path)\n",
    "\n",
    "    # Filter the data\n",
    "    data_filtered = filter_data(df_in)\n",
    "\n",
    "    # Split the dataset into sequences\n",
    "    datasets = split_dataset(data_filtered)\n",
    "\n",
    "    # Create empty lists to store the transformed epochs and their labels\n",
    "    all_transformed_epochs = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Process each dataset\n",
    "    for dataset in datasets:\n",
    "        # Get the epochs and labels for the current dataset\n",
    "        epochs, labels = get_epochs_and_labels(dataset)\n",
    "        \n",
    "        # Process each epoch\n",
    "        for epoch, label in zip(epochs, labels):\n",
    "            \n",
    "            # Z-score normalize each epoch\n",
    "            epoch_norm = z_score(epoch)\n",
    "        \n",
    "            # Apply wavelet transformation, on the normalized epoch\n",
    "            epoch_wavelet = apply_wavelet_transform(epoch_norm)\n",
    "            \n",
    "            # Append the transformed epoch and its label to the lists\n",
    "            all_transformed_epochs.append(epoch_wavelet)\n",
    "            all_labels.append(label)\n",
    "            \n",
    "    # Convert lists to NumPy arrays before creating tensors\n",
    "    all_transformed_epochs = np.array(all_transformed_epochs)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Convert the NumPy arrays to tensors\n",
    "    tensor_dataset = torch.tensor(all_transformed_epochs, dtype=torch.float)\n",
    "    labels_tensor = torch.tensor(all_labels, dtype=torch.long)\n",
    "        \n",
    "    # Get the current date and time\n",
    "    current_datetime = datetime.now()\n",
    "\n",
    "    # Format the date and time as a string for the dataset name\n",
    "    time_stamp = current_datetime.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Specify the output folder\n",
    "    output_folder = \"users\"\n",
    "\n",
    "    #Get the filename without the extension\n",
    "    user_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "\n",
    "    # Make a subfolder for the tensor dataset in the output folder, name it with the current date and time\n",
    "    subfolder = os.path.join(output_folder, user_name)    \n",
    "\n",
    "    # Create the subfolder if it does not exist\n",
    "    os.makedirs(subfolder, exist_ok=True)\n",
    "\n",
    "    # Save the combined dataset to the subfolder with the specified name\n",
    "    tensor_dataset_file_name = os.path.join(subfolder, f\"dataset_{time_stamp}.pt\")\n",
    "    torch.save(tensor_dataset, tensor_dataset_file_name)\n",
    "\n",
    "    labels_tensor_file_name = os.path.join(subfolder, f\"labels_{time_stamp}.pt\")\n",
    "    torch.save(labels_tensor, labels_tensor_file_name)\n",
    "    \n",
    "    return tensor_dataset, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\School\\EE_Y3\\Q4\\BAP\\eeg_thesis_cnn_repo\\data\\calibration\\raw\\training_both.csv\"\n",
    "\n",
    "tensor_dataset, labels_tensor = process_user_dataset(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
